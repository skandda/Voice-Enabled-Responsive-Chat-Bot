{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWOEpgE4oZYH"
      },
      "source": [
        "## Tutorial: How to Create a Voice-Enabled Chat-Bot\n",
        "\n",
        "**Team:** Skandda Chandrasekar, Tonya Chivandire, Luke Summers\n",
        "\n",
        "**Summary:** Our project is about creating a voice-enabled chat-bot that you can ask various questions to and receive accurate answers. Many chat-bots that are implemented today are all text-based bots and don't allow for people who may be blind or visually impaired to access them. As a result, we sought to create one that had increased accessibility for people who may not be able to use other chat-bots. Also, other related works focus on non-ML supported solutions and only allow for one question back and forth. By the end of this tutorial, you will be able to have a rolling audio conversation with a chat-bot that remembers all previous questions and answers.\n",
        "\n",
        "**Audience:** Individuals with beginner Python experience who want to learn about audio input and ouput as well as response generation, more specifically with the ChatGPT API.\n",
        "\n",
        "**External libraries used:** `SpeechRecognition`, `PyAudio`, `OpenAI`, `gTTS`, `playsound`, `os`, and potentially `flac` and/or `portaudio`\n",
        "\n",
        "**Vocabulary:** Natural Language Processing, ChatGPT, API, audio, API Key, class, model, chat-bot, system, library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zay7DmsKoZYJ"
      },
      "source": [
        "## Introduction\n",
        "This tutorial is written in Python and makes use of various libraries and APIs supported by Python.\n",
        "\n",
        "**What is a library?:** A library is a group of modules that can be downloaded and used in new programs. For example, one library used in this tutorial is the gTTS library, which provides functions for converting text into an audio file. Once downloaded, a library can be imported into any python file. This can be the whole library, but individual modules are able to be imported as well.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [What are Libraries in Python](https://codeinstitute.net/global/blog/what-are-libraries-in-python/)</span>\n",
        "\n",
        "**What is an API?:** An API or an application programming interface is a way to connect to other softwares in code. APIs can be used for various things, but in the case of this tutorial the OpenAI API will be used in order to query prompts into a ChatGPT model. Some APIs are what is called open which means they are free to use, with only an email account required to gain access to a key. OpenAI is not like this, as each query of the API requires a key with a connected account that it can charge. You will not have to worry about any of that for this tutorial, as significant payment would only arise for a far greater amount of queries than would be processed in running this tutorial multiple times. For any API, once you have a key you can use that to query the API in your code as long as your key has the proper access for your given call.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [What is an API](https://aws.amazon.com/what-is/api/)</span>\n",
        "\n",
        "Through the use of a library and API that converts the input from a microphone into text, the OpenAI ChatGPT API, and a library that converts text into a playable audio file, this tutorial will walk through creating a ChatGPT powered conversation bot in python.\n",
        "\n",
        "In this tutorial, we will discuss how to do the following:\n",
        "\n",
        "* Install a library onto your machine\n",
        "* Create an OpenAI account\n",
        "* Process audio into text (either from user input or a file)\n",
        "* Generate a ChatGPT-created response to a query\n",
        "* Convert said response to an audio file and output it\n",
        "* Have your chat-bot remember past questions and answers, like a conversation\n",
        "\n",
        "To start, we will learn how to install all the required dependencies for this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhnVXJnHoZYK"
      },
      "source": [
        "## Part 0: Installing Homebrew for Mac\n",
        "**Important Note:** If you are running this tutorial on Mac, you will need to install the the package manager Homebrew.\n",
        "\n",
        "<span style=\"color:salmon\">**I. Installing Homebrew**</span>\n",
        "\n",
        "Homebrew is a terminal package manager for Mac that we will utilize to install some required dependencies for installing certain libraries later in the tutorial. In order to install homebrew, point your browser to https://docs.brew.sh/Installation for homebrew's system requirements and installation instructions. After following the installation prompts, homebrew will be ready to use in the terminal. For this project, you will need to run the commands ```brew install flac``` and ```brew install portaudio``` in an instance of your MacBook's terminal. Also, homebrew installs things into different locations depending if your MacBook has an Intel or Apple produced processor. While this will not affect our project, if you were using homebrew to install packages for something like compiling c++ files, you may need to adjust your compiler to account for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0v1CccCoZYK"
      },
      "source": [
        "## Part 1: Installing Required Libraries\n",
        "\n",
        "**Important Note:** This tutorial will be run on Python 3.11.6 in a Jupyter Notebook environment. Again, we recommand running this through VScode.\n",
        "\n",
        "<span style=\"color:salmon\">**I. Processing Audio to Text**</span>\n",
        "\n",
        "The libraries required for this portion of the tutorial are SpeechRecognition and PyAudio. These can be installed using pip.\n",
        "Those who are utilizing MacBooks for this tutorial require both the Flac and PortAudio libraries that can be installed using homebrew. If you do not have homebrew on your machine, you can follow the instructions [here](https://brew.sh/).\n",
        "\n",
        "For MacBook users, please make sure you followed Part 0 in order to get flac and portaudio in your terminal.\n",
        "* ```brew install flac```\n",
        "* ```brew install portaudio```\n",
        "\n",
        "Both Windows and Mac users should run the following two commands\n",
        "\n",
        "* ```pip install SpeechRecognition```\n",
        "* ```pip install PyAudio```\n",
        "\n",
        "<span style=\"color:salmon\">**II. Generate Response from Text**</span>\n",
        "\n",
        "There is only one library required for this portion of the tutorial and it is OpenAI. This can be installed using pip.\n",
        "\n",
        "* ```pip install openai```\n",
        "\n",
        "<span style=\"color:salmon\">**II. Generate Response from Text**</span>\n",
        "\n",
        "The libraries required for this portion of this tutorial are Google Text to Speech and Playsound. These can be installed using pip. Please note that we will be installing a specific version of Playsound, specifically 1.2.2, which is compatible with the tutorial.\n",
        "\n",
        "* ```pip install gtts```\n",
        "* ```pip install playsound==1.2.2```\n",
        "\n",
        "If you have any trouble installing any library, but more specifically the Playsound library, try running the following command.\n",
        "\n",
        "* ```pip install --upgrade wheel```\n",
        "\n",
        "<span style=\"color:salmon\">If you want to automatically install these, run the following cell below. If you are on MacBook, make sure that you followed Part 0 in order to have flac and portaudio installed. Make sure to restart the Kernel in your code editor in order to use these libraries.</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtgyJrnooZYK"
      },
      "outputs": [],
      "source": [
        "%pip install SpeechRecognition\n",
        "%pip install PyAudio\n",
        "%pip install openai\n",
        "%pip install gTTS\n",
        "%pip install playsound==1.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYrVn1PmoZYL"
      },
      "source": [
        "## Troubleshooting\n",
        "<span style=\"color:salmon\">**I. Installing Anaconda**</span>\n",
        "\n",
        "If you have trouble running part one and get errors related to the flac library, try changing the kernel to anaconda. This can be done by installing anaconda and then restarting VS Code. When you try to run the code after restarting, choose the Anaconda python environment.\n",
        "Anaconda provides distributions of the Python and R programming languages, and it helps manage the packes for these environments and deploy them. While its focus is on running programs that involve data science, we will utilize it to change the python environment of the IPython notebook kernel. In order to install Anaconda, point your browser to https://www.anaconda.com/download, and download the Mac Anaconda installer. Once downloaded, run the installer following all of its prompts until Anaconda has successfully been installed. Once installed, the installer can be disposed of. In order to make use of the Anaconda environment in the IPython notebook kernel, upon the first run of code of an open notebook, you should be prompted to choose a python environment for the kernel. With Anaconda installed, there will be an option to choose an Anaconda environment instead of the recommended Python one, and choosing that option sets up the kernel successfuly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2I0oqHnoZYL"
      },
      "outputs": [],
      "source": [
        "#if anything fails to install, run this cell and see if it helps. If not, ignore this cell.\n",
        "%pip install --upgrade wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt-H7FYLoZYM"
      },
      "outputs": [],
      "source": [
        "#MACBOOK: Try this cell if you have issues later on retrieving audio. You could just install this to be safe as well\n",
        "%pip install -U PyObjC\n",
        "%pip install AppKit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiFxpoaJoZYM"
      },
      "source": [
        "## Part 2: Creating a OpenAI ChatGPT Account\n",
        "\n",
        "If you have created a ChatGPT account with your phone number longer than three months ago, then your free trial period for the API has expired, and you can just use our key. If you have access to a new phone number for a new ChatGPT account or have created an account within the past three months, then you can use your key in limited amounts for free.\n",
        "\n",
        "<span style=\"color:salmon\">**Our API Key:**</span> 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\n",
        "\n",
        "<span style=\"color:salmon\">**Follow the steps below to create an OpenAI ChatGPT Account and access your API key.**</span>\n",
        "\n",
        "Please note, this tutorial will be free for those who create a new ChatGPT Account and complete the tutorial within the first three months of doing so. You are also welcome to use our API key in this tutorial.\n",
        "\n",
        "Follow these steps to create a new account if you have not created one yet. If you have, then you should use our API key.\n",
        "\n",
        "1. Navigate in any internet browser to [OpenAI](https://openai.com).\n",
        "\n",
        "2. Click 'login' in the top right of the webpage.\n",
        "\n",
        "3. Create an account if you do not have one already. If you do, please login.\n",
        "\n",
        "4. Once you are logged in, navigate to the [documentation for OpenAI](https://platform.openai.com/docs/overview).\n",
        "\n",
        "5. On the left sidebar, navigate to the 'API Keys' section.\n",
        "\n",
        "6. Create a new 'secret key' and make sure that you copy down the key. Keep it in a safe place for later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XR-Lt_moZYM"
      },
      "source": [
        "## Part 3: Implementation: Processing Audio into Text\n",
        "\n",
        "There are two main processes involved in processing audio into text. First, we need to **retrieve the audio**. This is capturing and processing the audio into a sequence of bytes. Second, we need to **transcribe the audio**. This is converting the sequence of bytes into text/natural language.\n",
        "\n",
        "To do this, we first import the speech recognition library and create a new instance of the ```Recognizer``` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G4-ZvnJoZYM"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "recognizer = sr.Recognizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_7_U8aZoZYM"
      },
      "source": [
        "### 3.1 Retrieving the Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ljuXA0oZYN"
      },
      "source": [
        "<span>**I. Listening to Audio with a Microphone**</span>\n",
        "\n",
        "One way of retrieving audio input is by listening to live speech through a microphone. To do this we use the ```Microphone``` class from the speech recognition library and declare it as our audio source. The ```listen``` function will process the audio and return an object of type ```AudioData```. This object stores a sequence of bytes representing audio samples.\n",
        "\n",
        "Since we will be making more calls to `listen`, we will wrap the code in a function `listen_to_audio` which returns the audio data from `listen`.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [SpeechRecognition Microphone Example](https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py#L7-L11)</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrH92GkoZYN"
      },
      "outputs": [],
      "source": [
        "# obtain audio from the microphone\n",
        "def listen_to_audio():\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Say something!\")\n",
        "        audio = recognizer.listen(source)\n",
        "        print(\"Audio retrieved.\")\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrdH45CkoZYN"
      },
      "source": [
        "**Adjusting Energy Threshold:** The energy threshold is the minimum audio energy that the recognizer will consider for recording speech. It has a default value of 300. A higher value will make the microphone more sensitive, which is suitable if you are speaking in a loud room or a room with a lot of ambience sound. Not adjusting the threshold in such an environment can lead to unintelligible audio during transcription in Part 3.2. If noise is an issue, you can try adjusting the value of the energy threshold in the cell below.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [SpeechRecognition Library Reference](https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst#recognizer_instanceenergy_threshold--300---type-float)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDta4co9oZYN"
      },
      "outputs": [],
      "source": [
        "#This is what has worked well for us\n",
        "recognizer.energy_threshold = 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLQYJVNnoZYN"
      },
      "outputs": [],
      "source": [
        "mic_audio = listen_to_audio()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZJlYkTnoZYN"
      },
      "source": [
        "<span>**II. Retrieving the Audio from a File**</span>\n",
        "\n",
        "Alternatively, we can recognize speech input from an audio file. This is useful when needing to recognize pre-recorded or separate audio inputs e.g. a series of requests in different languages.\n",
        "\n",
        "To do this, we first declare our audio file as our source. Then we use the ```record``` function to read the entire audio file. Similarly to the ```listen``` function, ```record``` returns an ```AudioData``` object. This is wrapped in our function `get_audio_from_file`.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [SpeechRecognition Audio Transcribe Example](https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py#L11-L14)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ig1AEHCoZYN"
      },
      "outputs": [],
      "source": [
        "# read the entire audio file\n",
        "def get_audio_from_file(audio_file):\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        return recognizer.record(source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgIICTTIoZYN"
      },
      "source": [
        "The SpeechRecognition library only supports WAV, AIFF and FLAC audio file types. You can use the wav files provided for you in this tutorial, but if you prefer to use your own speech, record your speech to an audio file, convert it to a supported file format, and save the file in the same directory as this notebook. Be sure to replace *`how_are_you.wav`* below with the name of your file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBP8mwCAoZYN"
      },
      "outputs": [],
      "source": [
        "file_audio_en = get_audio_from_file(\"how_are_you.wav\")\n",
        "file_audio_fr = get_audio_from_file(\"french.flac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWGO6a77oZYN"
      },
      "source": [
        "### 3.2 Transcribing the Audio to Text\n",
        "\n",
        "Our audio is now retrieved, but as mentioned before, it is still stored in ```AudioData``` objects which is a sequence of bytes. We need to convert those bytes into actual text (a string) that we can feed as input to our chatbot. To do this, we can call the Google Speech Recognition API `recognize_google` to transcribe the audio data. This tutorial will use the default API key which does not need to be passed into the API call.\n",
        "\n",
        "Run the cell below to transcribe the speech that we recorded from the microphone earlier.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [SpeechRecognition Audio Transcribe Example](https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py#L24-L29)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR73KT_1oZYN"
      },
      "outputs": [],
      "source": [
        "mic_audio_text = recognizer.recognize_google(mic_audio)\n",
        "print(\"Google Speech Recognition thinks you said: \\\"{}\\\"\".format(mic_audio_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ujZixzCoZYN"
      },
      "source": [
        "The code cell below transcribes our two English and French audio files. Note that the default language is *en-US* and we need to specify the language parameter to recognize any other language/dialect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UAaBMIwoZYN"
      },
      "outputs": [],
      "source": [
        "text_en = recognizer.recognize_google(file_audio_en)\n",
        "print(\"Google Speech Recognition thinks you said: \\\"{}\\\"\".format(text_en))\n",
        "\n",
        "text_fr = recognizer.recognize_google(file_audio_fr, language=\"fr\")\n",
        "print(\"Google Speech Recognition thinks you said: \\\"{}\\\"\".format(text_fr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIz8tV02oZYN"
      },
      "source": [
        "The `recognize_google` function can raises two errors.\n",
        "\n",
        "1. An `UnknownValueError` is raised if the speech is unintelligible. This could be because the audio input is in a different language from the recognizer language (in which case you would specify the language as shown above). It could also be due to ambient noise (revisit 3.1 on how to adjust energy threshold).\n",
        "\n",
        "2. A `RequestError` might be a network issue (in which case check that your internet is stable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B4hvmNcoZYO"
      },
      "source": [
        "## Part 4: Implementation: Generating Response from Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnvG2ja0oZYO"
      },
      "source": [
        "At this point, we have taken audio input from the user and saved it as a string named 'user_input', and now we will use the ChatGPT API to get a conversational response to this string. In this section, we will write a function that takes this string as an input and queries the gpt-3.5-turbo ChatGPT model for a response, and then returns that response as a string. This will make use of the OpenAI object from the openai API, and will then use the .chat.completions functionality of the OpenAI object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHldHpj2oZYO"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtb3S0ZEoZYO"
      },
      "source": [
        "The first step is to create a variable for the API key. This will be used to make the call to the ChatGPT model for a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl39YMdloZYO"
      },
      "outputs": [],
      "source": [
        "#Replace this with your API key if you have generated one.\n",
        "OPENAI_API_KEY = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1VSzWZfoZYO"
      },
      "source": [
        "The function, `generate_response`, only takes in one string input, which is the input that is to be used on the ChatGPT query. The first step within the funtion is to make an OpenAI object, as with that we will be able to query whichever ChatGPT model we please. The API key is used as a parameter in the object construction as a way to unlock access to the API. Then, we will create a chat completion object in order to generate and save a response to our input. We then access and return the response that the query gave from the choices attribute of the chat completion object.\n",
        "\n",
        "The client (in our case) will take the model, and a list of inputs that are messages. We will manipulate these heavier later in the tutorial, so we will better explain it then. For now, just know we have a system command and then the question that the user asked. The system command just tells how the model to behave when creating responses.\n",
        "\n",
        "The call returns the response with lots more information than we need for the sake of this tutorial. For example, it gives a finish reason, id, and more. We are interested in the message content. We access this by calling `response.choices[0].message.content` as you can see in the return statement.\n",
        "\n",
        "Especially in this section, if you want to delve into the details and power of the API beyond what we are doing here, we suggest you read more on the OpenAI documentation pages.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [OpenAI API Documentation](https://platform.openai.com/docs/guides/text-generation)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jV7j63xoZYO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_response(question):\n",
        "  #construct OpenAI objct with API key\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "  #create chat compleations object whit 'question' as an input\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\", #sets the model of ChatGPT to desired model\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, #message from the system to the model that it is a helpful assistant\n",
        "      {\"role\": \"user\", \"content\": question}, #input message from user\n",
        "    ]\n",
        "  )\n",
        "  #\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MedxNfwKoZYO"
      },
      "source": [
        "This is an example call of the function that saves the response to a string variable named 'response'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC3op3udoZYO"
      },
      "outputs": [],
      "source": [
        "# generate response\n",
        "response = generate_response(mic_audio_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUv37h9YoZYO"
      },
      "source": [
        "## Part 5: Implementation: Converting Text to Speech and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMOSY2fyoZYO"
      },
      "source": [
        "We have now received a response to our initial voice-inputted question from ChatGPT. Reminder that we have saved this response in a local variable `response`, as you can see above. Now we can take this response and output it as audio through the device we are currently on! We will be using the `gTTS` ( _Google Text-To-Speech_), `playsound`, and `os` libraries to make this process trivial.\n",
        "\n",
        "There are four main parts in this part of the implementation:\n",
        "* Convert the response into a \"gTTS\" object\n",
        "* Save this object to an mp3 file in your current directory\n",
        "* Output the mp3 file as audio through the default audio player\n",
        "* Delete the mp3 file to clean up the process\n",
        "\n",
        "First, let's import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DTL_j4loZYO"
      },
      "outputs": [],
      "source": [
        "# Imports the required libraries to output the response as audio\n",
        "\n",
        "from gtts import gTTS\n",
        "import playsound\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YBCbLnwoZYO"
      },
      "source": [
        "Now that we have imported the required libraries, we can finally write our `text_to_speech` function that will complete the three steps outlined above. This function will take 4 inputs, but only one of them is required (the response generated in **\"Part 4\"** of the tutorial).\n",
        "\n",
        "These inputs are:\n",
        "* <span style=color:lightblue>response:</span> a string - the question response returned by the ChatGPT API\n",
        "* <span style=color:lightblue>language:</span> a string - default set to English, the language that you want the output to be. A comprehensive list of all languages can be found [here](https://gtts.readthedocs.io/en/latest/module.html#module-gtts.tts).\n",
        "* <span style=color:lightblue>speed:</span> a boolean - default set to false, will output audio slower if set to true\n",
        "* <span style=color:lightblue>file_name:</span> a string - default set to \"response.mp3\", the name of the temporary audio file that will hold your audio\n",
        "\n",
        "This function will not return anything. Instead, it will output the audio through your default computer audio player.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [gTTS Documentation](https://gtts.readthedocs.io/en/latest/), [playsound Documentation](https://pypi.org/project/playsound/)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p6hh3qOoZYR"
      },
      "outputs": [],
      "source": [
        "def text_to_speech(response, language='en', speed='False', file_name='response.mp3'):\n",
        "\n",
        "    #Uses the response to save a \"gTTS\" object\n",
        "    audio = gTTS(response, lang=language, slow=speed)\n",
        "\n",
        "    #Saves the object into your current directory using the desired file_name\n",
        "    audio.save(file_name)\n",
        "\n",
        "    #Get absolute path and format whitespaces\n",
        "    abs_path = os.path.abspath(file_name).replace(' ', '%20')\n",
        "\n",
        "    #Output the mp3 file as audio\n",
        "    playsound.playsound(abs_path)\n",
        "\n",
        "    #Delete the mp3 file\n",
        "    os.remove(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2VDJRsuoZYR"
      },
      "source": [
        "We have finished writing all the required functions for our voice-enabled chat-bot. We can now test our output by calling our `text_to_speech` function and receiving the answer to our question through audio. Run the cell below and get the answer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASUsKY28oZYR"
      },
      "outputs": [],
      "source": [
        "#Outputs the response as audio\n",
        "text_to_speech(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MddKbOm7oZYS"
      },
      "source": [
        "## Part 6: Implementation: Have a Recurring Conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkuGrIbGoZYS"
      },
      "source": [
        "Although we've finished the main part of this tutorial, a potential expansion upon a simple question and answer bot is to allow for a recurring conversation. That is, having the bot remember the previous quesitons that were asked so that it has context for future responses. We will do this by making edits to our `generate_response` function that we wrote earlier. Instead, we will write a function called `generate_rolling_response` that takes a log of messages into account as opposed to just a question.\n",
        "\n",
        "Besides that, the function will also take an OpenAI client as an input so that we aren't reinitializing a client every time we ask a new question in the thread.\n",
        "\n",
        "The inputs are:\n",
        "\n",
        "* <span style=color:lightblue>log:</span> a list - containing dictionaries with role and content, which will be explained later.\n",
        "* <span style=color:lightblue>client:</span> an OpenAI client - initialized with an API_KEY, this will generate your responses\n",
        "\n",
        "This function will return the response as a string.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [OpenAI API Documentation](https://platform.openai.com/docs/guides/text-generation)</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSo4rgqBoZYS"
      },
      "outputs": [],
      "source": [
        "#This function will feed a log with context to the client in order to get a more accurate response with context\n",
        "def generate_rolling_response(log, client):\n",
        "\n",
        "  #contacts the API to get a response\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=log\n",
        "  )\n",
        "\n",
        "  #returns the actual answer as a string\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnegFsTYoZYS"
      },
      "source": [
        "Now that we have updated our `generate_response` function to `generate_rolling_response`, we can write a function that will continuously ask for questions as well as output the responses as we did before. Now we can write a function called `chatbot_interface` in order to keep this conversation going at all times.\n",
        "\n",
        "We will initialize our client before entering the loop. You may be wondering what the log is; It is a list of dictionaries that contain the context for each entry to the chatbot. It contains a role and content.\n",
        "\n",
        "There are three different roles you can have, and the content usually will depend on the role:\n",
        "\n",
        "* <span style=color:lightblue>system:</span> This is basically describing how you want the chatbot to behave.\n",
        "* <span style=color:lightblue>user:</span> For our sake, this is the role that will be asking questions, i.e. you are the user\n",
        "* <span style=color:lightblue>assistant:</span> This is the chat-bot that will be generating responses based off how it is told to by the system.\n",
        "\n",
        "Please note that if the conversation goes on for too long, you will hit the ChatGPT token limit. In order to error check this, we will also write a function that will check the number of tokens in the log, called `count_tokens`. This function will take the log and then calculate the number of tokens, given the log. It will return the number of tokens in the current thread.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [Counting Tokens in Paragraph](https://www.tutorialspoint.com/python_text_processing/python_counting_token_in_paragraphs.htm)</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkFnymqDoZYS"
      },
      "outputs": [],
      "source": [
        "#genarically counts the number of tokens in the log to ensure that we don't hit the limit\n",
        "def count_tokens(log):\n",
        "    total_tokens = 0\n",
        "\n",
        "    #loops through all the dictionaries in the log and concatenates the tokens in the role and content keys\n",
        "    for query in log:\n",
        "        total_tokens += (len(query.get(\"role\")) + len(query.get(\"content\").split()))\n",
        "\n",
        "    return total_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4ZlM6PloZYS"
      },
      "source": [
        "To go more in-depth into the `chatbot_interface` function, we will initialize the log with the system command prior to starting our thread. We will use a generic \"You are a helpful assistant.\" for the content here so the bot generates typical responses. Then, we will enter the while loop which is the main part of the interface. This is where we will be talking back and forth with the chat-bot.\n",
        "\n",
        "* First, it will retrieve the input like before, but will check to see if the user says quit. That way, there is a way to break out of the thread. Then, it adds that user question to the log.\n",
        "* Then, it will check to make sure that the conversation is not too long that the model cannot deal with it. If it is, it breaks out of the thread.\n",
        "* If not, it generates a response with the log by giving the context to the client. After it does that, it adds the response given to the log as context.\n",
        "* Lastly, we output the last response as audio so the user gets the answer to the most recently asked question.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [OpenAI Documentation Chat Completions](https://platform.openai.com/docs/guides/text-generation/chat-completions-api), [OpenAI Token Limit](https://platform.openai.com/docs/models/gpt-3-5)</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWb_G78oZYS"
      },
      "outputs": [],
      "source": [
        "def chatbot_interface(max_tokens=4040):\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    #initializes the log with the system command\n",
        "    log = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "    #main loop\n",
        "    while(1):\n",
        "\n",
        "        #retrieve audio\n",
        "        print(\"Listening... Say quit to exit.\")\n",
        "        audio_input = listen_to_audio()\n",
        "        user_input = recognizer.recognize_google(audio_input)\n",
        "\n",
        "        #checks if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        print(\"You said: \" + user_input)\n",
        "\n",
        "        #appends the question to the log\n",
        "        log.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        #checks the number of tokens currently in the log, breaking out of the loop if it exceeds a soft value\n",
        "        tokens = count_tokens(log)\n",
        "        if tokens >= max_tokens:\n",
        "            print(\"Token limit exceeded, please start a new conversation.\")\n",
        "            break\n",
        "\n",
        "        #generates a response\n",
        "        response = generate_rolling_response(log, client)\n",
        "\n",
        "        #adds the response to the log\n",
        "        log.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        #outputs the response as audio\n",
        "        text_to_speech(response)\n",
        "\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sm9oeRxoZYS"
      },
      "source": [
        "Now, we can run our code to have a conversation with the chat-bot who will now remember previous questions it was asked as well as answers it has given. Run the following cell and test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm68pM8SoZYS"
      },
      "outputs": [],
      "source": [
        "recognizer.energy_threshold = 700\n",
        "\n",
        "chatbot_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7VbVewOoZYS"
      },
      "source": [
        "If you are struggling to retrieve the audio, try moving to a quieter location or messing with the threshold. Thank you so much for completing this tutorial with us!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTetQabtoZYS"
      },
      "source": [
        "## Part 7: Exploration: System Commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXpZIYPoZYS"
      },
      "source": [
        "We've completed the main part of the tutorial - We can now have a full audio conversation with a chat-bot that can generate responses to our questions. Now, what if we messed with the system command to get special styles of responses? To do this, we will edit the `chatbot_interface` function to the take a system command as a string. This way, if we have a specific use for our chat-bot, we can tune our system command to follow that.\n",
        "\n",
        "Our `chatbot_interface` function, on top of taking an optional input of <span style=color:lightblue>max_tokens</span>, we will add another optional input of <span style=color:lightblue>system_command</span>.\n",
        "\n",
        "We will call this funtion `custom_chatbot_interface`. Mess with the cell that calls this function. Try different system commands here and see how the output changes.\n",
        "\n",
        "<span style=color:lightblue>**Source:** [OpenAI Documentation Chat Completions](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7qc5NdvoZYS"
      },
      "outputs": [],
      "source": [
        "def custom_chatbot_interface(max_tokens=4040, system_command=\"You are a helpful assistant.\"):\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    #initializes the log with the system command\n",
        "    log = [{\"role\": \"system\", \"content\": system_command}]\n",
        "\n",
        "    #main loop\n",
        "    while(1):\n",
        "\n",
        "        #retrieve audio\n",
        "        print(\"Listening... Say quit to exit.\")\n",
        "        audio_input = listen_to_audio()\n",
        "        user_input = recognizer.recognize_google(audio_input)\n",
        "\n",
        "        #checks if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        print(\"You said: \" + user_input)\n",
        "\n",
        "        #appends the question to the log\n",
        "        log.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        #checks the number of tokens currently in the log, breaking out of the loop if it exceeds a soft value\n",
        "        tokens = count_tokens(log)\n",
        "        if tokens >= max_tokens:\n",
        "            print(\"Token limit exceeded, please start a new conversation.\")\n",
        "            break\n",
        "\n",
        "        #generates a response\n",
        "        response = generate_rolling_response(log, client)\n",
        "\n",
        "        #adds the response to the log\n",
        "        log.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        #outputs the response as audio\n",
        "        text_to_speech(response)\n",
        "\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cRdH9GDoZYS"
      },
      "outputs": [],
      "source": [
        "#insert your command here!\n",
        "#EXAMPLE: command = \"You are a very mean assistant.\"\n",
        "command = \"\"\n",
        "\n",
        "custom_chatbot_interface(system_command=command)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
